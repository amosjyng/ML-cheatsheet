\documentclass[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage{amsmath}
\usepackage[landscape]{geometry}

% To make this come out properly in landscape mode, do one of the following
% 1.
%  pdflatex latexsheet.tex
%
% 2.
%  latex latexsheet.tex
%  dvips -P pdf  -t landscape latexsheet.dvi
%  ps2pdf latexsheet.ps


% If you're reading this, be prepared for confusion.  Making this was
% a learning experience for me, and it shows.  Much of the placement
% was hacked in; if you make it better, let me know...


% 2008-04
% Changed page margin code to use the geometry package. Also added code for
% conditional page margins, depending on paper size. Thanks to Uwe Ziegenhagen
% for the suggestions.

% 2006-08
% Made changes based on suggestions from Gene Cooperman. <gene at ccs.neu.edu>


% To Do:
% \listoffigures \listoftables
% \setcounter{secnumdepth}{0}


% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
	{ \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}

% Turn off header and footer
\pagestyle{empty}
 

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}


% -----------------------------------------------------------------------

\begin{document}

\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{\textbf{Machine Learning Cheat Sheet}} \\
\end{center}

\section{Estimating Parameters}

\subsection{Maximum Likelihood Estimation (MLE)}

\begin{equation*}
	\underset{\theta}{\argmax} \ P(D | \theta)
\end{equation*}

\subsection{Maximum a Posteriori (MAP)}

\begin{equation*}
	\underset{\theta}{\argmax} \ P(\theta | D)
	= \underset{\theta}{\argmax} \ \frac{P(D | \theta) P(\theta)}{P(D)}
	= \underset{\theta}{\argmax} \ P(D | \theta) P(\theta)
\end{equation*}

\section{Regression}

\subsection{Linear Regression}

Find optimal $w$ and $b$ such that the average distance of points from the line is minimized:
\begin{equation*}
\underset{w,b}{\argmax} \ \sum_{i=1}^{n} \frac{1}{2} \left( w x_i + b - y_i \right)^2
\end{equation*}
Differentiate, and solution is
\begin{equation*}
\left[ \sum_{i = 1}^{n} x_i x_i^T \right] w = \sum_{i = 1}^{n} y_i x_i
\end{equation*}

$y$ as Gaussian:
\begin{align*}
y_i &= \theta^T x_i + \epsilon\\
\epsilon &\sim N(0, \gamma^2)\\
P(\epsilon_i) &= \frac{1}{\sqrt{2\pi} \sigma} exp \left( - \frac{\epsilon_i^2}{2 \sigma^2} \right)\\
P(y_i | x_i, \theta) &= \frac{1}{\sqrt{2 \pi} \sigma} exp \left( - \frac{(y_i - \theta^T x_i)^2}{2 \sigma^2} \right)
\end{align*}

Eigenvalues of $x^T x$ are positive. Use $x^T M x \geq 0 \:\forall x \in R^n$ to prove this:
\begin{align*}
v^T \sum_{i} x_i x_i^T v \geq 0\\
\sum_{i} (v^T x_i)^2 \geq 0
\end{align*}

\emph{Note} that when the data does not span the entire space, $x^T x$ does not have full rank, and thus is non-invertible. This can be fixed by either decomposition through removing eigenvalues below a certain threshold, or adding a small $\epsilon$ to the diagonal of the matrix.

\section{Classification}

\subsection{Naive Bayes}

Assumes that features are independent of one another given the class. Thus, learning and classification for each feature happens independently of other features.

\emph{Classifies} input vector $<x_1, ..., x_n>$ as class $y$ according to
\begin{align*}
	\underset{y}{\argmax} \ P(y | x_1, ..., x_n)
	&= \underset{y}{\argmax} \ P(x_1, ..., x_n | y) P(y)\\
	&= \underset{y}{\argmax} \ P(y) \prod_{i}^{n} P(x_i | y)
\end{align*}

\emph{Learns} $P(x_i | y)$ for
\begin{itemize}
	\item \emph{Discrete $x_i$} -- $P(x_i | y) = \frac{\#D(X_i = x_i, Y = y)}{\#D(Y = y)}$
	
	For smoothing, use $P(x_i | y) = \frac{\#D(X_i = x_i, Y = y) + k}{\#D(Y = y) + n_i k}$, where $n_i$ is the number of different possible values for $X_i$
	\item \emph{Continuous $x_i$} -- Can use any PDF, but usually use Gaussian $P(x_i | y) = \mathcal{N}(\mu_{X_i | y}, \sigma_{X_i | y}^2)$, where $\mu_{X_i | y}$ and $\sigma_{X_i | y}$ are, respectively, the average and variance of $X_i$ for all data points where $Y = y$. The Gaussian distribution already provides smoothing.
\end{itemize}

\subsection{Perceptron}

Produces linear decision boundaries.

\emph{Classifies} using $\hat{y} = X_{test}\:w +  b$

\emph{Learns} $w$ and $b$ by updating $w$ whenever $y_i (w^T x_i + b) \leq 0$ (i.e. incorrectly classified). Updates as such:
\begin{align*}
w &\leftarrow w + x_i y_i\\
b &\leftarrow b + y_i
\end{align*}
Repeat until all examples are correctly classified.

$w$ is some linear combination $\sum_{i} \alpha_i x_i$ of data points, and decision boundary is the linear hyperplane $f(x) = w^T x + b$.

Note that the perceptron is the same as stochastic gradient descent with a hinge loss function of $max(0, 1 - y_i [<w, x_i> + b])$

\subsubsection{Kernelized Perceptron}

If $y_i (\sum_{j \in I} y_j k(x_j, x_i) + b) \leq 0$, add $i$-th data point to list $I$ and $b := b + y_i$.

\subsubsection{Convergence}

Assume
\begin{itemize}
	\item $\exists w^*$ such that $||w^*|| = 1$
	\item $\exists \gamma > 0$ such that $\forall t = 1 ... n,\ y_t(x_t \cdot w^*) \geq \gamma$
	\item $\forall t = 1 ... n,\ ||x_t|| \leq R$
\end{itemize}

Then the perceptron algorithm makes at most $R^2 / \gamma^2$ errors.

Proof
\begin{itemize}
	\item Bound $w^{k + 1} \cdot w^* \rightarrow ||w^{k + 1}|| \geq k \gamma$
	\item Bound $||w^{k + 1}||^2
	$\begin{align*}
		k R^2 &\geq ||w^{k + 1}||^2 \geq k^2 \gamma^2\\
		\frac{R^2}{\gamma^2} &\geq k
	\end{align*}
\end{itemize}
If we don't include $b$ as part of $w$, this becomes
\begin{equation*}
	k \leq \frac{(b^* + 1) (r^2 + 1)}{\gamma^2}
\end{equation*}

\subsection{SVM}

Minimize $\frac{1}{2} ||w||^2$ such that $y_i (w^T x_i + b) \geq 1,\: i = 1 ... m$
\begin{align*}
g_i(w) &= -y_i (w^T x_i + b) + 1 \leq 0\\
L(w, b, \alpha) &= \frac{1}{2} ||w||^2 - \sum_{i = 1}^{m} \alpha_i \left[ y_i (w^T x_i + b) - 1 \right]\\
\nabla_w' L &= w - \sum_{i = 1}^{m} \alpha_i y_i x_i = 0\\
\frac{\delta}{\delta b} L &= \sum \alpha_i y_i = 0
\end{align*}
The dual form:
\begin{align*}
\underset{\alpha}{\max} \sum_{i = 1}^{m} \alpha_i &- \frac{1}{2} \sum_{i,j=1}^{m} y_i y_j \alpha_i \alpha_j <x_i, x_j>\\
\alpha_i &\geq 0\\
\sum \alpha_i y_i &= 0
\end{align*}
Kernelized version:
\begin{align*}
w^T x + b &= (\sum \alpha_i y_i x_i)^T x + b\\
&= \sum a_i y_i <x_i, x> + b
\end{align*}
Solving for $b$ for $\alpha_k > 0$ gives $b = y_k - <w, x_k>$.

\emph{Note} that $L(x^*, \lambda) \leq L(x^*, \lambda^*) \leq L(x, \lambda^*)$

\subsubsection{SVM with soft margins}

Minimize $\frac{1}{2} ||w||^2 + C \sum \xi_i$, subject to constraints $y_i (w^T x_i + b) \geq 1 - \xi_i$ and $\xi_i \geq 0$.

The dual is maximizing $\sum \alpha_i - (1/2) \sum_i \sum_j y_i y_j \alpha_i \alpha_j <x_i, x_j>$ subject to $0 \leq \alpha \leq c$ and $\sum \alpha_i y_i = 0$.

\subsection{kNN}

Given a region where $\hat{p}(x)$ of the data points are in the majority class, there is a $\hat{p}$ probability of getting a test point correct. Error rate is $1 - \hat{p}(x)$. As the \# of neighbors go to $\infty$ and as $n \rightarrow \infty$, $k \in \ensuremath{\mathcal{O}}(\sqrt{n})$. Error for 1NN is $2\hat{p} (1 - \hat{p})$ and error for kNN is $\min(\hat{p}, 1 - \hat{p})$.

\subsection{Cross-validation}

\subsubsection{LOOCV}

\begin{align*}
\log p(x_i | X - x_i) &= \log \frac{1}{n - 1} \sum_{j \neq i} k(x_i, x_j)\\
p(x_i) &= \frac{1}{n} \sum k(x_i, x)\\
\frac{1}{n} \sum_{i = 1}^{n} &\log \left[ \frac{n}{n - 1} p(x_i) - \frac{1}{n - 1} k(x_i, x_i) \right]\\
\end{align*}

\subsubsection{k-fold}

Error is for $(k - 1) / k$ of the training set.

\section{Optimization}

\subsection{Linear Programming}

Minimize $c^T x$ subject to $A x \leq b$ and $x \geq 0$. Dual is max $b^T \alpha$ subject to $A^T \alpha + c = 0$ and $\alpha \geq 0$.

\subsection{Quadratic Programming}

Minimize $f(x) = \frac{1}{2} x^T Q x + c^T x$ subject to either
\begin{itemize}
	\item $A x + d\leq 0$ (inequality constraint)
	\item $E x = f$ (equality constraint)
\end{itemize}
Dual for the inequality constraint is
\begin{equation*}
\underset{\alpha}{\min} \frac{1}{2} \alpha^T A Q^{-1} A^T \alpha + \alpha^T [A Q^{-1} c - d]
\end{equation*}
subject to $\alpha \geq 0$

\subsection{Gradient Descent}

Guess $x_0$ to be a local minimum of $F(x)$, then obtain $x_{n + 1} = x_n - \gamma_n \nabla F(x_n)$, where $n \geq 0$. Either use fixed step size $t$ or use line search to find optimal $\gamma_n$. If $F$ is convex, then this converges to the global solution. Usually we take $F = (1/h) \sum l(x_i; M)$ to be the loss function, and we try to minimize the loss function.

$k \leq \frac{M}{m} log \frac{f(x) - f(x^*)}{\epsilon}$, where $m$ is the parabola underneath and $M$ is the parabola over $f$, which is bounded by the two.

\begin{align*}
\beta_j &:= \beta_j - \alpha \frac{\delta}{\delta \beta_j} J(\beta)\\
J(\beta) &= \frac{1}{2} \sum_{1}^{n} (\beta^T x - y)^2\\
\frac{\delta J(\beta)}{\beta_i} &= (\beta^T x - y) x_i\\
\beta &= (x^T x)^{-1} x^T \vec{y}\\
\end{align*}

\subsubsection{Gradient Descent Line Search}

Gradient descent in one dimension. Converges according to
\begin{equation*}
t \geq log(A - B) - log(\epsilon)
\end{equation*}

\subsubsection{Stochastic Gradient Descent}

Randomly shuffle data points, and then repeat for $i = 1 ... n$, $\theta_{t + 1} := \theta_t - \alpha \delta_\theta (y_i, <\phi(x_i), \theta_t>)$ until convergence, where $\delta_\theta(y_i, <\phi(x_i), \theta>) = \frac{\delta}{\delta \theta} \left[ \frac{1}{2} (y_i - <x_i, w>)^2 \right]$ (essentially taking the derivative of the loss function).

It's randomized -- run $n$ times and pick best. Reaches close to but not quite the optimum.
\begin{equation*}
\textmd{Regret} = \textmd{Current Perf.} - \textmd{Best possible perf.} < \ensuremath{\mathcal{O}}(1/\sqrt{t})
\end{equation*}
$\alpha$ is the learning rate. The best possible performance is when the loss function is strongly convex, in which case regret is $< \ensuremath{\mathcal{O}}(1/T)$

\subsection{Newton's Method}

Choose an initial point that is close enough to $x^*$, then update $\beta := \beta - f'(x)/f''(x)$ or $\beta := \beta - H^{-1}(l(\beta)) \nabla_\beta l(\beta)$. Every iteration is more expensive than GD, but converges faster within a bound.

Converges quadratically in the region near the optimal value, given that the Hessian can be found and that $f(x)$ is twice differentiable. Inside the bound,
\begin{equation*}
||x_{n + 1} - x^*|| \leq \gamma || \left[ \delta_x^2 f(x_n) \right]^{-1} || ||x_n - x^*||^2
\end{equation*}
Outside the bound,
\begin{equation*}
|| \delta_x f(x^*) - \delta_x f(x) - <x^* - x, \delta_x^2 f(x)> || \leq \gamma ||x^* - x||^2
\end{equation*}
and it is as bad as gradient descent.

BFGS approximates Newton's method. It computes the Hessian by approximating the gradient, so computation of $H^{-1}$ is not required.

\section{Parzen Window}

\begin{align*}
p_{emp}(x) &= \frac{1}{n} \sum_{i = 1}^{n} \delta_{xi}(x)\\
\hat{p}(x) &= \frac{1}{n} \sum_{i = 1}^{n} k_{xi}(x)
\end{align*}

Doing MLE on Parzen $\hat{p}(x) \rightarrow \infty$ as $k \rightarrow \infty$, so do leave-one-out estimation.

Conditions for a smoothing kernel are $\int k(x) dx = 1$, $\int x k(x) dx = 0$, and $\int x^2 k(x) dx > 0$.

\subsection{Loss functions}

\begin{tabular}{lc}
Hinge & $l(y, f(x)) = \max(0, 1 - y \cdot f(x))$\\
Huber & $ l(y, f(x)) =
	\begin{cases}
		\hfill 0 \hfill & \textmd{if } y \cdot f(x) > 1\\
		\hfill \frac{1}{2} (1 - y f(x))^2 \hfill & \textmd{if } y \cdot f(x) \in [0,1]\\
		\hfill \frac{1}{2} - y f(x) \hfill & \textmd{if } y \cdot f(x) < 0
	\end{cases} $\\
Binary & $l(y, f(x)) = {y \cdot f(x) < 0}$\\
Logistic & $l(y, f(x)) = \log (1 + \exp(- y \cdot f(x)))$
\end{tabular}
\emph{Note} that ``hinge'' is also called ``soft margin.''

\section{Duality / KKT}

The Karush-Kuhn-Tucker conditions are
\begin{itemize}
	\item \emph{Primal feasibility:} Satisfies original constraints
	\item \emph{Dual feasibility:} $\nabla_{\alpha_i} L(x^*, \alpha^*) = c_i(x^*) \leq 0$ (saddle point in $\alpha^*$)
	\item \emph{Complementary slackness:} $\sum_{i} \alpha_i^* c_i(x^*) = 0$ (vanishing KKT gap)
	\item \emph{Stationarity:} $\nabla_x L(x^*, a^*) = \nabla_x f(x^*) + \sum \alpha_i^* \nabla_x c_i(x^*) = 0$ (saddle point in $x^*$)
\end{itemize}
where $L = f(x) + \lambda c(x)$, where $c(x)$ is a matrix of the constraints. Under strong duality (values of Lagrangian is strictly less than value of primal), if these conditions are satisfied, then the solution of the dual is equal to the solution of the primal.

To solve a Lagrangian multipliers problem, first find $L$, then take the partial of $L$ with respect to every variable of $L$ and set these partials to 0. Substitute the variables back into $L$ to get an equation without those original variables and only the $\lambda$'s. Now the goal is to maximize this expression, subject to the constraints $\lambda_i \geq 0$ and the partials equaling 0.

\subsection{Convexity}

Set $C$ is convex if $\lambda x + (1 - \lambda) y \in C\;\forall x,y \in C,\:\lambda \in [0,1]$

Function $f$ is convex if its epigraph is a convex set; i.e. all these three conditions are equivalent:
\begin{align*}
f(\lambda x + (1 - \lambda) y) &\leq \lambda f(x) + (1 - \lambda) f(y), \; \lambda \in [0,1]\\
f(y) - f(x) &\geq f'(x) (y - x), \; x,y \in \textmd{dom }f\\
f''(x) &\geq 0
\end{align*}

If $f(x)$ and $g(x)$ are convex, then $a f(x)$, $f(Ax + b)$, $f(x) + g(x)$, $max(f(x), g(x))$, and $g(f(x))$ are also convex.

\subsubsection{Strict convexity}

Has unique minima. Here the Hessian condition is sufficient but \emph{not} necessary for strict convexity.
\begin{align*}
f(\lambda x + (1 - \lambda) y) &< \lambda f(x) + (1 - \lambda) f(y), \; \lambda \in [0,1]\\
f(y) - f(x) &> f'(x) (y - x), \; x,y \in \textmd{dom }f\\
f''(x) &> 0
\end{align*}

\subsubsection{Strong convexity}

If $m$ is the curvature coefficient, then
\begin{align*}
f(\lambda x + (1 - \lambda) y) &\leq \lambda f(x) + (1 - \lambda) f(y) - \frac{m}{2} \lambda (1 - \lambda) ||x - y||_2^2\\
f(y) - f(x) &\geq f'(x) (y - x) + \frac{m}{2} ||y - x||_2^2\\
f''(x) &\geq mI
\end{align*}

\subsubsection{Convex Hull}

Given a convex set $C$, the convex hull is $\{\bar{x} | \bar{x} = \sum_{i = 1}^{n} \alpha_i x_i,\;x_i \in C,\}$ for any values of $\alpha_i$ satisfying $\sum_{i = 1}^{n} \alpha_i \leq 1$.

$\underset{x}{sup} f(x) = \underset{x \in Co_f}{sup} f(x)$ where $Co_f$ is the convex hull of $f$.

\section{Matrix Differentiation}

\begin{align*}
\frac{\partial (x - A s)^T W (x - A s)}{\partial s} &= - 2 A^T W (x - A s)\\
\frac{\partial |x^T A x|}{\partial x} &= |x^T A x| (A x (x^T A x)^{-1} + A^T x (x^T A x)^{-1})\\
\frac{\partial ((x a + b)^T c (x a + b))}{\partial x} &= (c + c^T) (x a + b) a^T
\end{align*}

\begin{tabular}{cc}
$\frac{\partial |A \times B|}{\partial x} = |A \times B| (x^T)^{-1}$ & $\frac{\partial \log |x|}{\partial x} = (x^T)^{-1} = (x^{-1})^T$\\
$\frac{\partial (a^T x)}{\partial x} = \frac{\partial (x^T a)}{\partial x} = a$ & $\frac{\partial (x^T A x)}{\partial x} = (A + A^T) x$\\
$\frac{\partial (a^T x b)}{\partial x} = a b^T$ & $\frac{\partial (a^T x^T b)}{\partial x} = b a^T$\\
$\frac{\partial (a^T x a)}{\partial x} = \frac{\partial (a^T x^T a)}{\partial x} = a a^T$ & $\frac{\partial (a^T x^T c x b)}{\partial x} = c^T x a b^T + c x b a^T$\\
\end{tabular}

\end{multicols}
\end{document}
